{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot using Seq2Seq LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- In this notebook, we will assemble a seq2seq LSTM model using Keras Functional API to create a working Chatbot which would answer questions asked to it.\n",
    "\n",
    "- Chatbots have become applications themselves. You can choose the field or stream and gather data regarding various questions. We can build a chatbot for an e-commerce webiste or a school website where parents could get information about the school.\n",
    "\n",
    "\n",
    "- The famous [Google Assistant](https://assistant.google.com/), [Siri](https://www.apple.com/in/siri/), [Cortana](https://www.microsoft.com/en-in/windows/cortana) and [Alexa](https://www.alexa.com/) may have been build using simialr models.\n",
    "\n",
    "So, let's start building our Chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importing the packages\n",
    "\n",
    "We will import [TensorFlow](https://www.tensorflow.org) and [Keras](https://www.tensorflow.org/guide/keras). Also, we import other modules which help in defining model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "import string\n",
    "import tensorflow as tf\n",
    "# NLTK\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Reading the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loading Data\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('qa_Electronics.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questionType</th>\n",
       "      <th>asin</th>\n",
       "      <th>answerTime</th>\n",
       "      <th>unixTime</th>\n",
       "      <th>question</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Dec 27, 2013</td>\n",
       "      <td>1.388131e+09</td>\n",
       "      <td>Is this cover the one that fits the old nook c...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes this fits both the nook color and the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Jan 5, 2015</td>\n",
       "      <td>1.420445e+09</td>\n",
       "      <td>Does it fit Nook GlowLight?</td>\n",
       "      <td>N</td>\n",
       "      <td>No. The nook color or color tablet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Would it fit Nook 1st Edition? 4.9in x 7.7in ?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I don't think so. The nook color is 5 x 8 so n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>17 days ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Will this fit a Nook Color that's 5 x 8?</td>\n",
       "      <td>Y</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Feb 10, 2015</td>\n",
       "      <td>1.423555e+09</td>\n",
       "      <td>will this fit the Samsung Galaxy Tab 4 Nook 10.1</td>\n",
       "      <td>N</td>\n",
       "      <td>No, the tab is smaller than the 'color'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Jan 30, 2015</td>\n",
       "      <td>1.422605e+09</td>\n",
       "      <td>does it have a flip stand?</td>\n",
       "      <td>N</td>\n",
       "      <td>No, there is not a flip stand. It has a pocket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Jan 30, 2015</td>\n",
       "      <td>1.422605e+09</td>\n",
       "      <td>does this have a flip stand</td>\n",
       "      <td>?</td>\n",
       "      <td>Hi, no it doesn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Dec 22, 2014</td>\n",
       "      <td>1.419235e+09</td>\n",
       "      <td>also fits the HD+?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It should. They are the same size and the char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes/no</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Nov 16, 2014</td>\n",
       "      <td>1.416125e+09</td>\n",
       "      <td>Does it have 2 positions for the reader? Horiz...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>open-ended</td>\n",
       "      <td>0594033926</td>\n",
       "      <td>Aug 7, 2014</td>\n",
       "      <td>1.407395e+09</td>\n",
       "      <td>Is there a closure mechanism? Bands, magnetic,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No- it is more like a normal book would be. It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  questionType        asin    answerTime      unixTime  \\\n",
       "0       yes/no  0594033926  Dec 27, 2013  1.388131e+09   \n",
       "1       yes/no  0594033926   Jan 5, 2015  1.420445e+09   \n",
       "2   open-ended  0594033926    2 days ago           NaN   \n",
       "3       yes/no  0594033926   17 days ago           NaN   \n",
       "4       yes/no  0594033926  Feb 10, 2015  1.423555e+09   \n",
       "5       yes/no  0594033926  Jan 30, 2015  1.422605e+09   \n",
       "6       yes/no  0594033926  Jan 30, 2015  1.422605e+09   \n",
       "7   open-ended  0594033926  Dec 22, 2014  1.419235e+09   \n",
       "8       yes/no  0594033926  Nov 16, 2014  1.416125e+09   \n",
       "9   open-ended  0594033926   Aug 7, 2014  1.407395e+09   \n",
       "\n",
       "                                            question answerType  \\\n",
       "0  Is this cover the one that fits the old nook c...          Y   \n",
       "1                        Does it fit Nook GlowLight?          N   \n",
       "2     Would it fit Nook 1st Edition? 4.9in x 7.7in ?        NaN   \n",
       "3           Will this fit a Nook Color that's 5 x 8?          Y   \n",
       "4   will this fit the Samsung Galaxy Tab 4 Nook 10.1          N   \n",
       "5                         does it have a flip stand?          N   \n",
       "6                        does this have a flip stand          ?   \n",
       "7                                 also fits the HD+?        NaN   \n",
       "8  Does it have 2 positions for the reader? Horiz...          Y   \n",
       "9  Is there a closure mechanism? Bands, magnetic,...        NaN   \n",
       "\n",
       "                                              answer  \n",
       "0  Yes this fits both the nook color and the same...  \n",
       "1                 No. The nook color or color tablet  \n",
       "2  I don't think so. The nook color is 5 x 8 so n...  \n",
       "3                                                yes  \n",
       "4            No, the tab is smaller than the 'color'  \n",
       "5  No, there is not a flip stand. It has a pocket...  \n",
       "6                                  Hi, no it doesn't  \n",
       "7  It should. They are the same size and the char...  \n",
       "8                                                Yes  \n",
       "9  No- it is more like a normal book would be. It...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove null questions and also questions which have character count less than 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "null_id = []\n",
    "for i, val in enumerate(df['question']):\n",
    "    if len(val) <= 10:\n",
    "        null_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  df.drop(df.index[null_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 312129 entries, 0 to 314262\n",
      "Data columns (total 7 columns):\n",
      "questionType    312129 non-null object\n",
      "asin            312129 non-null object\n",
      "answerTime      312129 non-null object\n",
      "unixTime        302749 non-null float64\n",
      "question        312129 non-null object\n",
      "answerType      165529 non-null object\n",
      "answer          312129 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39371"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['asin'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['questionType','asin','answerTime','unixTime','answerType'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is this cover the one that fits the old nook c...</td>\n",
       "      <td>Yes this fits both the nook color and the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does it fit Nook GlowLight?</td>\n",
       "      <td>No. The nook color or color tablet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Would it fit Nook 1st Edition? 4.9in x 7.7in ?</td>\n",
       "      <td>I don't think so. The nook color is 5 x 8 so n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will this fit a Nook Color that's 5 x 8?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will this fit the Samsung Galaxy Tab 4 Nook 10.1</td>\n",
       "      <td>No, the tab is smaller than the 'color'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Is this cover the one that fits the old nook c...   \n",
       "1                        Does it fit Nook GlowLight?   \n",
       "2     Would it fit Nook 1st Edition? 4.9in x 7.7in ?   \n",
       "3           Will this fit a Nook Color that's 5 x 8?   \n",
       "4   will this fit the Samsung Galaxy Tab 4 Nook 10.1   \n",
       "\n",
       "                                              answer  \n",
       "0  Yes this fits both the nook color and the same...  \n",
       "1                 No. The nook color or color tablet  \n",
       "2  I don't think so. The nook color is 5 x 8 so n...  \n",
       "3                                                yes  \n",
       "4            No, the tab is smaller than the 'color'  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is this cover the one that fits the old nook c...</td>\n",
       "      <td>yes this fits both the nook color and the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>does it fit nook glowlight?</td>\n",
       "      <td>no. the nook color or color tablet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would it fit nook 1st edition? 4.9in x 7.7in ?</td>\n",
       "      <td>i don't think so. the nook color is 5 x 8 so n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>will this fit a nook color that's 5 x 8?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>will this fit the samsung galaxy tab 4 nook 10.1</td>\n",
       "      <td>no, the tab is smaller than the 'color'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>does it have a flip stand?</td>\n",
       "      <td>no, there is not a flip stand. it has a pocket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>does this have a flip stand</td>\n",
       "      <td>hi, no it doesn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>also fits the hd+?</td>\n",
       "      <td>it should. they are the same size and the char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>does it have 2 positions for the reader? horiz...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is there a closure mechanism? bands, magnetic,...</td>\n",
       "      <td>no- it is more like a normal book would be. it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  is this cover the one that fits the old nook c...   \n",
       "1                        does it fit nook glowlight?   \n",
       "2     would it fit nook 1st edition? 4.9in x 7.7in ?   \n",
       "3           will this fit a nook color that's 5 x 8?   \n",
       "4   will this fit the samsung galaxy tab 4 nook 10.1   \n",
       "5                         does it have a flip stand?   \n",
       "6                        does this have a flip stand   \n",
       "7                                 also fits the hd+?   \n",
       "8  does it have 2 positions for the reader? horiz...   \n",
       "9  is there a closure mechanism? bands, magnetic,...   \n",
       "\n",
       "                                              answer  \n",
       "0  yes this fits both the nook color and the same...  \n",
       "1                 no. the nook color or color tablet  \n",
       "2  i don't think so. the nook color is 5 x 8 so n...  \n",
       "3                                                yes  \n",
       "4            no, the tab is smaller than the 'color'  \n",
       "5  no, there is not a flip stand. it has a pocket...  \n",
       "6                                  hi, no it doesn't  \n",
       "7  it should. they are the same size and the char...  \n",
       "8                                                yes  \n",
       "9  no- it is more like a normal book would be. it...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the count of questions with word length less than 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions:  138103\n",
      "Total Anwers:  88087\n"
     ]
    }
   ],
   "source": [
    "cnt = [i for i in df2['question'] if  len(i.split()) <= 10]\n",
    "print(\"Total Questions: \",len(cnt))\n",
    "cnt = [i for i in df2['answer'] if  len(i.split()) <= 10]\n",
    "print(\"Total Anwers: \",len(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the index of questions which has word length above 7 to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # .find(\"soccer\")\n",
    "# cnt =0\n",
    "# for i, val in enumerate(df2['answer']):\n",
    "#     if val.find(\"http:\")!= -1:\n",
    "#         print(i, \" :  \",val)\n",
    "#         cnt+=1\n",
    "\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_qn_ans(col_type):\n",
    "    data_id = []\n",
    "    for i, val in enumerate(df2[col_type]):\n",
    "        if len(val.split()) >= 10 :\n",
    "            data_id.append(i)\n",
    "            \n",
    "    print('Count of ',col_type,' will be removed: ',len(data_id))\n",
    "    return data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_qn_ans_alternate(col_type):\n",
    "    data_id = []\n",
    "    checker = 0\n",
    "    for i, val in enumerate(df2[col_type]):\n",
    "        if len(val) <= 3 and checker < 2:\n",
    "            data_id.append(i)\n",
    "            checker += 1\n",
    "        elif val.find(\"http:\")!= -1:\n",
    "            data_id.append(i)\n",
    "        else:\n",
    "            checker = 0\n",
    "            \n",
    "    print('Count of ',col_type,' will be removed: ',len(data_id))\n",
    "    return data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of  question  will be removed:  190590\n",
      "Count of  answer  will be removed:  79094\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.drop(df2.index[remove_qn_ans('question')])\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "df2 = df2.drop(df2.index[remove_qn_ans('answer')])\n",
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of  answer  will be removed:  9911\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.drop(df2.index[remove_qn_ans_alternate('answer')])\n",
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does it fit nook glowlight?</td>\n",
       "      <td>no. the nook color or color tablet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>does this have a flip stand</td>\n",
       "      <td>hi, no it doesn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how far out does the arm extend?</td>\n",
       "      <td>18 inches on our tv.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does this item come with a charger?</td>\n",
       "      <td>ours did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>does this version have a camera?</td>\n",
       "      <td>no, nook glowlight does not have a camera.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32529</th>\n",
       "      <td>how many watts is this speaker?</td>\n",
       "      <td>3 watt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32530</th>\n",
       "      <td>how much does this speaker weigh (without the ...</td>\n",
       "      <td>almost 7 oz with the strap to carry it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32531</th>\n",
       "      <td>can you use this laptop on dsl + satellite</td>\n",
       "      <td>yep, it doesn't have a cd rom though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32532</th>\n",
       "      <td>does it have a cd drive ?</td>\n",
       "      <td>no. it has no cd/dvd drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32533</th>\n",
       "      <td>does it have a built in cam</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32534 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0                            does it fit nook glowlight?   \n",
       "1                            does this have a flip stand   \n",
       "2                       how far out does the arm extend?   \n",
       "3                    does this item come with a charger?   \n",
       "4                       does this version have a camera?   \n",
       "...                                                  ...   \n",
       "32529                    how many watts is this speaker?   \n",
       "32530  how much does this speaker weigh (without the ...   \n",
       "32531         can you use this laptop on dsl + satellite   \n",
       "32532                          does it have a cd drive ?   \n",
       "32533                        does it have a built in cam   \n",
       "\n",
       "                                           answer  \n",
       "0              no. the nook color or color tablet  \n",
       "1                               hi, no it doesn't  \n",
       "2                            18 inches on our tv.  \n",
       "3                                        ours did  \n",
       "4      no, nook glowlight does not have a camera.  \n",
       "...                                           ...  \n",
       "32529                                      3 watt  \n",
       "32530     almost 7 oz with the strap to carry it.  \n",
       "32531       yep, it doesn't have a cd rom though.  \n",
       "32532                  no. it has no cd/dvd drive  \n",
       "32533                                         yes  \n",
       "\n",
       "[32534 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['does it fit nook glowlight?',\n",
       " 'does this have a flip stand',\n",
       " 'how far out does the arm extend?',\n",
       " 'does this item come with a charger?',\n",
       " 'does this version have a camera?',\n",
       " 'does this nook play games',\n",
       " 'does this model have an sd card slot?',\n",
       " 'how well can you see the screen in sunlight?',\n",
       " 'can i download the kindle app for this?',\n",
       " 'can you download netflix?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions =  df2['question'].values.tolist()\n",
    "# questions = questions[0:len(questions)//2]\n",
    "questions = questions[0:10100]\n",
    "print(len(questions))\n",
    "questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['no. the nook color or color tablet',\n",
       " \"hi, no it doesn't\",\n",
       " '18 inches on our tv.',\n",
       " 'ours did',\n",
       " 'no, nook glowlight does not have a camera.',\n",
       " 'no it does not .... sorry',\n",
       " 'yes it has an sd card slot',\n",
       " 'in the shades ok direct sun not so good',\n",
       " 'yes you can. i have it installed',\n",
       " 'yes, you can also use the amazon kindle app.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = df2['answer'].values.tolist()\n",
    "answers = answers[0:10100]\n",
    "print(len(answers))\n",
    "answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average character length of questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does this have a flip stand\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(questions[1])\n",
    "print(len(questions[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length of questions:  71\n"
     ]
    }
   ],
   "source": [
    "length =0\n",
    "for i in questions:\n",
    "    length += len(i)\n",
    "    \n",
    "print('average length of questions: ',length//4879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length of answers:  52\n"
     ]
    }
   ],
   "source": [
    "length =0\n",
    "for i in answers:\n",
    "    length += len(i)\n",
    "    \n",
    "print('average length of answers: ',length//4879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str :\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes.',\n",
       " 'yes, it works on any mac',\n",
       " \"why wouldn't it?\",\n",
       " 'yes... this will work all over the globe.',\n",
       " 'it has worked fine for us.',\n",
       " 'it work perfectly thanks',\n",
       " 'i understand that it will work in it fine.',\n",
       " 'yes.',\n",
       " \"yes. it's on my phone, it's good.\",\n",
       " 'yes sir.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_with_tags[15:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> yes. <END>',\n",
       " '<START> yes, it works on any mac <END>',\n",
       " \"<START> why wouldn't it? <END>\",\n",
       " '<START> yes... this will work all over the globe. <END>',\n",
       " '<START> it has worked fine for us. <END>',\n",
       " '<START> it work perfectly thanks <END>',\n",
       " '<START> i understand that it will work in it fine. <END>',\n",
       " '<START> yes. <END>',\n",
       " \"<START> yes. it's on my phone, it's good. <END>\",\n",
       " '<START> yes sir. <END>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[15:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` ) into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 8476\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### C) Preparing data for Seq2Seq model\n",
    "\n",
    "Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n",
    "\n",
    "For `encoder_input_data` :\n",
    "* Tokenize the `questions`. Pad them to their maximum length.\n",
    "\n",
    "For `decoder_input_data` :\n",
    "* Tokenize the `answers`. Pad them to their maximum length.\n",
    "\n",
    "For `decoder_output_data` :\n",
    "\n",
    "* Tokenize the `answers`. Remove the first element from all the `tokenized_answers`. This is the `<START>` element which we added earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 15) 15\n"
     ]
    }
   ],
   "source": [
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   7,    4,   15,   10, 1331,  292,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_questions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 26) 26\n"
     ]
    }
   ],
   "source": [
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_output_data -1 \n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14,    5,  919, ...,    0,    0,    0],\n",
       "       [ 141,   14,    3, ...,    0,    0,    0],\n",
       "       [ 461,   87,   17, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  18,   71,   66, ...,    0,    0,    0],\n",
       "       [  13,  126,    3, ...,    0,    0,    0],\n",
       "       [  10, 8474, 8475, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10100, 26, 8476)\n"
     ]
    }
   ],
   "source": [
    "# # decoder_output_data -2\n",
    "\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "# onehot_answers = sparse_categorical_crossentropy( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.isfile(\"./encoder_input_data.npy\"):\n",
    "    print(\"Loading existing numpy file: \")\n",
    "    # load model\n",
    "    encoder_input_data = np.load('encoder_input_data.npy')\n",
    "    decoder_input_data = np.load('decoder_input_data.npy')\n",
    "    decoder_output_data = np.load('decoder_output_data.npy')\n",
    "else:\n",
    "    # Saving all the arrays to storage\n",
    "    np.save( 'encoder_input_data.npy' , encoder_input_data )\n",
    "    np.save( 'decoder_input_data.npy' , decoder_input_data )\n",
    "    np.save( 'decoder_output_data.npy' , decoder_output_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the arrays to storage\n",
    "np.save( 'encoder_input_data.npy' , encoder_input_data )\n",
    "np.save( 'decoder_input_data.npy' , decoder_input_data )\n",
    "np.save( 'decoder_output_data.npy' , decoder_output_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Defining the Encoder-Decoder model\n",
    "\n",
    "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
    "\n",
    "\n",
    "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
    "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
    "*   LSTM layer : Provide access to Long-Short Term cells.\n",
    "\n",
    "Working : \n",
    "\n",
    "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
    "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
    "3.   These states are set in the LSTM cell of the decoder.\n",
    "4.   The decoder_input_data comes in through the Embedding layer.\n",
    "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
    "\n",
    "**Important points :**\n",
    "\n",
    "\n",
    "*   `200` is the output of the GloVe embeddings.\n",
    "*   `embedding_matrix` is the GloVe embedding which we downloaded earlier.\n",
    "\n",
    "\n",
    "<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n",
    "\n",
    "\n",
    "Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kaila\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\kaila\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\kaila\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    1695200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    1695200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8476)   1703676     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,735,676\n",
      "Trainable params: 5,735,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train / Save / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: \n",
      "Train on 10100 samples\n",
      "Epoch 1/50\n",
      "10100/10100 [==============================] - 31s 3ms/sample - loss: 1.5918\n",
      "Epoch 2/50\n",
      "10100/10100 [==============================] - 19s 2ms/sample - loss: 1.3542\n",
      "Epoch 3/50\n",
      "10100/10100 [==============================] - 23s 2ms/sample - loss: 1.2658\n",
      "Epoch 4/50\n",
      "10100/10100 [==============================] - 23s 2ms/sample - loss: 1.2114\n",
      "Epoch 5/50\n",
      "10100/10100 [==============================] - 24s 2ms/sample - loss: 1.1668\n",
      "Epoch 6/50\n",
      "10100/10100 [==============================] - 28s 3ms/sample - loss: 1.1273\n",
      "Epoch 7/50\n",
      "10100/10100 [==============================] - 33s 3ms/sample - loss: 1.0918\n",
      "Epoch 8/50\n",
      "10100/10100 [==============================] - 35s 3ms/sample - loss: 1.0609\n",
      "Epoch 9/50\n",
      "10100/10100 [==============================] - 39s 4ms/sample - loss: 1.0324\n",
      "Epoch 10/50\n",
      "10100/10100 [==============================] - 45s 4ms/sample - loss: 1.0055\n",
      "Epoch 11/50\n",
      "10100/10100 [==============================] - 42s 4ms/sample - loss: 0.9809\n",
      "Epoch 12/50\n",
      "10100/10100 [==============================] - 38s 4ms/sample - loss: 0.9573\n",
      "Epoch 13/50\n",
      "10100/10100 [==============================] - 29s 3ms/sample - loss: 0.9353\n",
      "Epoch 14/50\n",
      "10100/10100 [==============================] - 23s 2ms/sample - loss: 0.9144\n",
      "Epoch 15/50\n",
      "10100/10100 [==============================] - 23s 2ms/sample - loss: 0.8943\n",
      "Epoch 16/50\n",
      "10100/10100 [==============================] - 22s 2ms/sample - loss: 0.8743\n",
      "Epoch 17/50\n",
      "10100/10100 [==============================] - 23s 2ms/sample - loss: 0.8555\n",
      "Epoch 18/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.8372\n",
      "Epoch 19/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.8187\n",
      "Epoch 20/50\n",
      "10100/10100 [==============================] - 21s 2ms/sample - loss: 0.8009\n",
      "Epoch 21/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.7835\n",
      "Epoch 22/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.7660\n",
      "Epoch 23/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.7495\n",
      "Epoch 24/50\n",
      "10100/10100 [==============================] - 21s 2ms/sample - loss: 0.7327\n",
      "Epoch 25/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.7165\n",
      "Epoch 26/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.7007\n",
      "Epoch 27/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.6846\n",
      "Epoch 28/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.6685\n",
      "Epoch 29/50\n",
      "10100/10100 [==============================] - 21s 2ms/sample - loss: 0.6528\n",
      "Epoch 30/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.6377\n",
      "Epoch 31/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.6222\n",
      "Epoch 32/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.6066\n",
      "Epoch 33/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5922\n",
      "Epoch 34/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5772\n",
      "Epoch 35/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5635\n",
      "Epoch 36/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5489\n",
      "Epoch 37/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5345\n",
      "Epoch 38/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5208\n",
      "Epoch 39/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.5066\n",
      "Epoch 40/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.4936\n",
      "Epoch 41/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.4808\n",
      "Epoch 42/50\n",
      "10100/10100 [==============================] - 21s 2ms/sample - loss: 0.4680\n",
      "Epoch 43/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.4552\n",
      "Epoch 44/50\n",
      "10100/10100 [==============================] - 21s 2ms/sample - loss: 0.4421\n",
      "Epoch 45/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.4303\n",
      "Epoch 46/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.4177\n",
      "Epoch 47/50\n",
      "10100/10100 [==============================] - 21s 2ms/sample - loss: 0.4051\n",
      "Epoch 48/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.3943\n",
      "Epoch 49/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.3824\n",
      "Epoch 50/50\n",
      "10100/10100 [==============================] - 20s 2ms/sample - loss: 0.3710\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "if os.path.isfile(\"./model_lstm_220.h5\"):\n",
    "    print(\"Loading existing model: \")\n",
    "    # load model\n",
    "    model = load_model('model_lstm_220.h5')\n",
    "    # summarize model.\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"Training model: \")\n",
    "    # Train model first\n",
    "    model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=220, epochs=50, use_multiprocessing= True) \n",
    "\n",
    "    # Creating pickel file\n",
    "    model.save( 'model_lstm_220.h5' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Defining inference models\n",
    "\n",
    "We create inference models which help in predicting answers.\n",
    "\n",
    "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
    "\n",
    "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Talking with our Chatbot\n",
    "\n",
    "First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "2.   We set the state values in the decoder's LSTM.\n",
    "3.   Then, we generate a sequence which contains the `<start>` element.\n",
    "4.   We input this sequence in the `dec_model`.\n",
    "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
    "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer length.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : does it has camera\n",
      " yes it does end\n",
      "Enter question : is it waterproof\n",
      " no it is not end\n",
      "Enter question : size of mobile\n",
      " 4 volts end\n",
      "Enter question : what are the specifications\n",
      " 20 watts end\n",
      "Enter question : can you download netflix\n",
      " yes we can ship to the computer end\n",
      "Enter question : what is size\n",
      " 4 5 x 3 4 x 1 4 end\n",
      "Enter question : how bright is it\n",
      " i don't know because it's about 5 feet end\n"
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "#             else:\n",
    "#                 decoded_translation = \"I am sorry! I don't understand you\"\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "        \n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
